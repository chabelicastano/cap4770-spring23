{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chabelicastano/cap4770-spring23/blob/main/Labs/ChabeliCastano_A_first_look_at_deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mKArb-b1_En"
      },
      "source": [
        "### Note:\n",
        "\n",
        "**First let's set the runtime to GPU -- click on 'runtime' in the menu above, select 'Change runtime type' and pick 'GPU'.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-n3xuc-8gykk",
        "outputId": "1c68d220-4b33-4876-f225-b5abbb2521f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Apr  3 19:29:36 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ii5clez0HmE"
      },
      "source": [
        "# A First Look at Deep Learning using Keras  \n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/keras22.png)\n",
        "\n",
        "\n",
        "\n",
        "This notebook is intended to be a first quick hands-on introduction to deep learning using TensorFlow and Keras. \n",
        "\n",
        "\n",
        "First, let's cover a few definitions.\n",
        "\n",
        "#### Tensor\n",
        "Tensors, like arrays and matrices, are containers for numbers. For now, it is fine to think of tensors as arrays. Tensors have three defining characteristics. \n",
        "\n",
        "1. **Number of Axes or Dimensions**\n",
        "2. **The shape**\n",
        "3. **The Type of Data Stored in the Tensor**\n",
        "\n",
        "For example, the tensor `x` below \n",
        "\n",
        "* has one axis (it is one dimensional).\n",
        "* its shape is represented by the tuple `(4)` --- there are 4 elements in that one axis.\n",
        "* the datatype is `uint8` (the tensor contains integers).\n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/tensors43.png)\n",
        "\n",
        "The tensor `y`\n",
        "\n",
        "* has 2 axis (2 dimensional)\n",
        "* its shape is (3,4). Along one axis there are 3 rows and along the other, 4 columns\n",
        "* the datatype is `uint8`\n",
        "\n",
        "Finally, the tensor `z` \n",
        "\n",
        "* has 3 axis   -- it is 3 dimensional\n",
        "* its shape is (2, 3, 4)\n",
        "* the datatype is `uint8`\n",
        "\n",
        "**Tensors are the fundamental datatype for deep learning systems.**\n",
        "\n",
        "\n",
        "### Keras\n",
        "Keras is open source software that functions as an abstract interface to TensorFlow. \n",
        "\n",
        "My difficulty ranking of deep learning libraries (from easiest to hardest) is \n",
        "\n",
        "1. Keras\n",
        "2. PyTorch\n",
        "3. TensorFlow\n",
        "\n",
        "All have their strengths and it is difficult to recommend which one a beginner should start with. The excellent, free, Open.ai course uses PyTorch. However, in our exploration of deep learning we will start with Keras. In your path toward deepening your knowledge in machine learning you will likely encounter all three.\n",
        "\n",
        "# A first look at a neural network\n",
        "Let's go back to the example of recognizing hand written digits. \n",
        "\n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/mmnist.png)\n",
        "\n",
        "We are using the MNIST dataset (Modified National Institute of Standards) which is a dataset of 60,000 training instances and 10,000 testing instances of 28x28 grayscale images of the digits 0 through 9. This dataset was created in the 1980s as a testbed for various research groups. Back then it was considered a hard problem. Today it is considered the \"hello world\" equivalent in deep learning and you will see it again and again on your path learning about machine learning. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHO0x8YE0HmF"
      },
      "source": [
        "\n",
        "\n",
        "# Prelim steps\n",
        "\n",
        "## 1. Import Keras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DS58igpC0HmG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5f823deb-85da-42ec-ab80-2a227eda99d9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.12.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import keras\n",
        "keras.__version__ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noY1wNno0HmK"
      },
      "source": [
        "\n",
        "\n",
        "## 2. Define the datasets we are going to use.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tUaQ5q20HmK"
      },
      "source": [
        "The MNIST dataset is so common that Keras knows how to download the data into a set of four Numpy arrays:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voWJ4eMm0HmL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2b1a927-c24d-49c5-f796-e2d8109b38ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsarChQu0HmN"
      },
      "source": [
        "`train_images` and `train_labels` form the \"training set\", the data that the model will learn from. The model will then be tested on the \n",
        "\"test set\", `test_images` and `test_labels`. The Keras version of this dataset encodes the images as Numpy arrays, and the labels are simply an array of digits, ranging \n",
        "from 0 to 9. There is a one-to-one correspondence between the images and the labels.\n",
        "\n",
        "## 3. Examining the data\n",
        "\n",
        "As already mentioned, each digit in the data is represented by a 28x28 pixel image:\n",
        "\n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/mnist2.png)\n",
        "\n",
        "This is represented as a 2 dimensional array of 28 rows and 28 columns. The gray scale at each location is represented by a number. Total white is represented by 0, total black as 255, and shades of gray are represented by numbers between those two. \n",
        "\n",
        "Just for a sanity check, let's see if this format matches a sample of our training data. Let's get the first batch of our data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5UJNpAs0HmN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "372d1b2f-4bed-4337-ff3d-11bb6a104d75"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "train_images.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SexwMUDfdF-"
      },
      "source": [
        "That is not surprising---so 60,000 images that are a 28x28 array of pixel values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGoBVIWU0HmQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "880d7b10-7dd3-4f29-9d7e-0789bd348d48"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "len(train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xrzLvCu0HmV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c9634b5-a216-43fd-e542-c4ccbb9b77e7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "train_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sw4IhMIr0Hmm"
      },
      "source": [
        "# The data\n",
        "To get an idea of what the data looks like let's display a few images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4oH15sT0Hmm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dff6114a-7aff-4c30-c796-fa120e4c828d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 200x200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 200x200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 200x200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 200x200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 200x200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def viewImage(x):\n",
        "    plt.figure(figsize=(2,2))\n",
        "    plt.imshow(x, interpolation='nearest', cmap='Greys')\n",
        "    plt.show()\n",
        "    \n",
        "viewImage(test_images[0])\n",
        "viewImage(test_images[1])\n",
        "viewImage(test_images[2])\n",
        "viewImage(test_images[3])\n",
        "viewImage(test_images[4])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ereJKgeH_-5f"
      },
      "source": [
        "and let's look at the associated labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8HKsdisAC_x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "014432ac-0337-4f5c-85a1-83ab476bcb0c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 2, 1, 0, 4], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "test_labels[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHPEFxanojmP"
      },
      "source": [
        "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/torchdivide.png)\n",
        "\n",
        "# Our deep learning network design.\n",
        "\n",
        "Let us now turn our attention to the deep learning network we will implement.\n",
        "\n",
        "The input to our deep learning network will be a 28x28 image. Instead of using the two dimensional 28 x 28 representation, for simplicity we will flatten the image into a sequence of 784 pixels (28 times 28). These 784 pixel values will be our input. There are 10 possible outputs representing the digits 0 through 9. Thus, the architecture of our input values and output nodes looks like this:\n",
        "\n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/dense1a.png)\n",
        "\n",
        "Between the input and the output we will have one or more hidden layers. *Hidden layers* are simply the layers of nodes between the input and the output. \n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/hiddenpyt.png)\n",
        "\n",
        "\n",
        "For our initial network we are going to have one hidden layer of 256 nodes. The layer will be densely connected (also called *fully connected*) meaning that each input  (in this case each pixel) is connected to each node in the inner layer:\n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/connect1p.png)\n",
        "\n",
        "\n",
        "So each of the 784 pixel values is connected to each of the 256 nodes of the hidden layer.\n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/connect2p.png)\n",
        "\n",
        "Let's examine one of these nodes in the hidden layer in more detail. Each node has 784 inputs and each of these inputs has an associated weight. So *input<sub>1</sub>* has an associated *weight<sub>1</sub>*, *input<sub>2</sub>* has an associated *weight<sub>2</sub>* and *input<sub>784</sub>* has an associated *weight<sub>784</sub>*. Also, each node has an additional weight *weight<sub>0</sub>*. \n",
        "\n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/relu2.png)\n",
        "\n",
        "Each node multiplies its input by the associated weights:\n",
        "\n",
        "$$\n",
        "total = \\sum_{i=0}^{784}{x_i w_i}\n",
        "$$\n",
        "\n",
        "Since the inputs are represented by a tensor and the weights are as well, this operation can be done very efficiently on the parallel architecture of a GPU. Next, the node determines its output based on this total weighted input, using an **activation function**. One common activation function is **ReLU** (rectified linear unit) and is defined by \n",
        "\n",
        "$$\n",
        "relu(total) = \\max(0, total)\n",
        "$$\n",
        "\n",
        "So the output of a single node is\n",
        "\n",
        "$$\n",
        "output = \\max(0, \\sum_{i=0}^{784}{x_i w_i})\n",
        "$$\n",
        "\n",
        "Since each node has 785 weights and there are 256 nodes in our hidden layer, there are 200,960 weights in that layer. \n",
        "\n",
        "\n",
        "#### Output layer\n",
        "The output layer is also densely connected, mean each node in the hidden layer will be connected to each of the output nodes. \n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/connect3p.png)\n",
        "\n",
        "\n",
        "Resulting in a dense, or fully connected network.\n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/connect4p.png)\n",
        "\n",
        "\n",
        "Each node in the output layer has 256 + 1 weights (256 because each node in the output layer is connected to each of the outputs of the 256 nodes in the hidden layer). Thus, there are 2,570 trainable parameters in the output layer for a total of 203,530 trainable parameters. These weights, or trainable parameters, are what the model learns during the training phase. Typically, these parameters are initially set to random numbers.\n",
        "\n",
        "The network in these images is shown with the input on the left and the output on the right and the processing proceeds left to right. The hidden layer performs the calculations described above Each node in the hidden layer has a specific output and these outputs are then processed by the output layer. If we had more layers to our network (for example, three hidden layers), the progression on calculations would be similar. When processing proceeds in this direction it is known as **forward propagation**. *Forward* meaning that the calculations proceed from the initial inputs to the final output.  Using forward propagation, the networks takes inputs representing an image of the number 4 as an example and the network makes a prediction which may be correct (*it is a 4*) or incorrect (*it is a 9*). In what is called a **back propagation** step, the system uses the actual label and the prediction to adjust the weights throughout the network. This will be described in more detail later.\n",
        "\n",
        "As FranÃ§ois Chollet, creator of Keras says:  \n",
        "\n",
        "> The core building block of neural networks is the \"layer\", a data-processing module which you can conceive as a \"filter\" for data. Some \n",
        "data comes in, and comes out in a more useful form. Precisely, layers extract _representations_ out of the data fed into them -- hopefully \n",
        "representations that are more meaningful for the problem at hand. Most of deep learning really consists of chaining together simple layers \n",
        "which will implement a form of progressive \"data distillation\". A deep learning model is like a sieve for data processing, made of a \n",
        "succession of increasingly refined data filters -- the \"layers\".\n",
        "\n",
        " \n",
        "\n",
        "#### The total network.\n",
        "This network image above represents the basic architecture of our system\n",
        "\n",
        "* 784 inputs (the pixels of the MNIST image)\n",
        "* 256 densely connected nodes of the hidden layer\n",
        "* 10 output nodes represented the 10 possible classifications of an image (the digits 0 through 9)\n",
        "\n",
        "### Implementing in Keras\n",
        "\n",
        "Just to introduce some code, let us look how to implement this architecture:\n",
        "\n",
        "```\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "input_size=784 \n",
        "hidden_size = 256\n",
        "output_size = 10\n",
        "\n",
        "network = models.Sequential()\n",
        "network.add(layers.Dense(hidden_size, activation='relu', input_shape=(input_size,)))\n",
        "network.add(layers.Dense(output_size, activation='softmax'))\n",
        "\n",
        "```\n",
        "\n",
        "First, we set up some variables representing, as the variable names suggest, the input size (the 28 by 28 image contains 784 pixels, the number of hidden nodes (256), and the number of classes in the output (10).\n",
        "\n",
        "Next, we define `network`. `Sequential` creates a sequential container, which means that the layers we add are organized sequentially.\n",
        "\n",
        "The `add` method adds a layer to the network.\n",
        "\n",
        "* `Layers.dense` creates a dense (fully connected) layer. \n",
        "  * The first parameter is the size of the hidden layer---in our case `hidden_size`\n",
        "  * `activation=relu,` specifies we are using the ReLU activation function.\n",
        "  * `input_shape` specifies, as the name suggests, the shape of the input. In our case the 784 pixels of the image.\n",
        "* `network.add(layers.Dense(output_size, activation='softmax'))` Next we add another densely connected layer to our network. \n",
        "  * `output_size` There will be 10 nodes in this layer. Each node represents one of the possible labels for the image--the digits 0 through 9.\n",
        "  * `activation='softmax',` specifies we are using the softmax activation function. This function will give us the probabilies of the possible labels. (For ex., 80% sure it is a '5', etc)\n",
        "\n",
        "\n",
        "If we instead wanted 64 nodes in our hidden layer our code would be\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "input_size=784 \n",
        "hidden_size = 64\n",
        "output_size = 10\n",
        "\n",
        "network = models.Sequential()\n",
        "network.add(layers.Dense(hidden_size, activation='relu', input_shape=(input_size,)))\n",
        "network.add(layers.Dense(output_size, activation='softmax'))\n",
        "\n",
        "```\n",
        "\n",
        "And if we wanted a network with two hidden layers , the first with 256 nodes and the second with 64 nodes we might code it...\n",
        "\n",
        "```\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "input_size=784 \n",
        "hidden_size = [256, 64]\n",
        "output_size = 10\n",
        "\n",
        "network = models.Sequential()\n",
        "network.add(layers.Dense(hidden_size[0], activation='relu', input_shape=(input_size,)))\n",
        "network.add(layers.Dense(hidden_size[1], activation='relu'))\n",
        "network.add(layers.Dense(output_size, activation='softmax'))\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_XKnAu30Hmp"
      },
      "source": [
        "\n",
        "### Workflow\n",
        "\n",
        "\n",
        "Our workflow will be as follow: \n",
        "\n",
        "1. we will create a neural network containing a fully connected hidden layer.\n",
        "2. we will present our neural network with the training data, `train_images` and `train_labels`. The \n",
        "network will then learn to associate images and labels by adjusting the 2,570 weights.\n",
        "3. we will ask the network to produce predictions for `test_images`\n",
        "4. we will verify if these predictions match the labels from `test_labels`.\n",
        "\n",
        "#### Let's build our network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6I0sj6hW0Hmp"
      },
      "outputs": [],
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "input_size=784 \n",
        "hidden_size = 256\n",
        "output_size = 10\n",
        "\n",
        "network = models.Sequential()\n",
        "network.add(layers.Dense(hidden_size, activation='relu', input_shape=(input_size,)))\n",
        "network.add(layers.Dense(output_size, activation='softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouiEONIe0Hms"
      },
      "source": [
        "\n",
        "The core building block of neural networks is the \"layer\", a data-processing module which you can conceive as a \"filter\" for data. Some \n",
        "data comes in, and comes out in a more useful form. Precisely, layers extract _representations_ out of the data fed into them -- hopefully \n",
        "representations that are more meaningful for the problem at hand. Most of deep learning really consists of chaining together simple layers \n",
        "which will implement a form of progressive \"data distillation\". A deep learning model is like a sieve for data processing, made of a \n",
        "succession of increasingly refined data filters -- the \"layers\".\n",
        "\n",
        "Here our network consists of a sequence of two `Dense` layers, which are densely-connected (also called \"fully-connected\") neural layers. \n",
        "The second (and last) layer is a 10-way \"softmax\" layer, which means it will return an array of 10 probability scores (summing to 1). Each \n",
        "score will be the probability that the current digit image belongs to one of our 10 digit classes.\n",
        "\n",
        "\n",
        "#### Compiling the model.\n",
        "To make our network ready for training, we need to pick three more things, as part of a compilation step:\n",
        "\n",
        "* An **optimizer**: this is the mechanism through which the network will update itself based on the data it sees and its loss function.\n",
        "* A **loss function**: this is how the network will be able to measure how good a job it is doing on its training data, and thus how it will be \n",
        "able to steer itself in the right direction.\n",
        "* **Metrics** to monitor during training and testing. Here we will only care about accuracy (the fraction of the images that were correctly \n",
        "classified).\n",
        "\n",
        "Keras makes this compilation step easy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXUIZUwN0Hmt"
      },
      "outputs": [],
      "source": [
        "network.compile(optimizer='rmsprop',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcclDPL10Hmu"
      },
      "source": [
        "\n",
        "Why did we select categorical crossentropy as the loss function? From the Tensorflow documentation:\n",
        "\n",
        "> Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided in a one_hot representation. If you want to provide labels as integers, please use SparseCategoricalCrossentropy loss. There should be # classes floating point values per feature.\n",
        "\n",
        "Note that the documentation says that we need to one_hot encode the labels. You probably already know this but that means if we are labeling pictures of dogs, cats, squirrel and moose and our test_labels look like\n",
        "\n",
        "picture | label\n",
        ":---:   | :---\n",
        "1.  | dog\n",
        "2. | dog\n",
        "3. | cat\n",
        "4. | squirel\n",
        "5. | cat\n",
        "6. | moose\n",
        "\n",
        "We will one_hot encode this resulting in \n",
        "\n",
        "\n",
        "picture | dog | cat | squirrel | moose\n",
        ":---: | :----: |:--: | :---: | :---:\n",
        " 1.     | 1 | 0 | 0 | 0\n",
        " 2. | 1 | 0| 0 | 0\n",
        " 3. | 0 | 1 | 0 | 0\n",
        " 4. | 0 | 0 | 1 | 0\n",
        " 5. | 0 | 1 |0 | 0\n",
        " 6. | 0|0|0|1\n",
        "\n",
        "And again, the `metrics=['accuracy']` line means that we are calculating the percentage of predicted values that match with actual values.\n",
        "\n",
        "\n",
        "### Preprocessing the data\n",
        "Before training, we will preprocess our data by reshaping it into the shape that the network expects, and scaling it so that all values are in \n",
        "the `[0, 1]` interval. Previously, our training images for instance were stored in an array of shape `(60000, 28, 28)` of type `uint8` with \n",
        "values in the `[0, 255]` interval. We transform it into a `float32` array of shape `(60000, 28 * 28)` with values between 0 and 1.\n",
        "\n",
        "In other words, each image in our original data was a 28x28 array of integers ranging from 0 to 255. We are going to transform the integers 0-255 to a float between 0 and 1. In addition we are going to flatten each image array to look like\n",
        "\n",
        "```\n",
        "[0, 1, 2, 3, 4, 5 ... 784 ]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-tevXPN0Hmv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a76323c9-3e9c-4fd3-c95c-60398b450ff2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "test_images[:5]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to see the format\n",
        "train_images[0]"
      ],
      "metadata": {
        "id": "Jyo-MnS1kFe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efw6Tx4x0Hmy"
      },
      "source": [
        "#### Here's a question:\n",
        "Why are we dividing each pixel by 255? What did the original number represent?\n",
        "\n",
        "#### Encode the labels\n",
        "We also need to categorically encode the labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBoQhwiX0Hmz"
      },
      "outputs": [],
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNSszmzz0Hm1"
      },
      "source": [
        "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
        "\n",
        "## <font color='#EE4C2C'>1. What does the first test label look like? (print it out)</font> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6de8eFdF0Hm1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aa14df7-670a-40df-c69f-a29e098951d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "test_labels[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3wWDY0g0Hm3"
      },
      "source": [
        "### Training our first deep learning model\n",
        "\n",
        "We are now ready to train our network, which, unsurprisingly, in Keras is done via a call to the `fit` method of the network: \n",
        "we \"fit\" the model to its training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JQ7pH630Hm4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acbbccd7-0595-419b-81c2-e64fd7f9ac40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "469/469 [==============================] - 6s 3ms/step - loss: 0.2981 - accuracy: 0.9156\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.1291 - accuracy: 0.9628\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0879 - accuracy: 0.9748\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0661 - accuracy: 0.9807\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0518 - accuracy: 0.9849\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f813726c9a0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "network.fit(train_images, train_labels, epochs=5, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37PUt_b-0Hm7"
      },
      "source": [
        "Two quantities are being displayed during training: the \"loss\" of the network over the training data, and the accuracy of the network over \n",
        "the training data.\n",
        "\n",
        "We quickly reach an accuracy of 0.989 (i.e. 98.9%) on the training data. Now let's check that our model performs well on the test set too:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ig8FrFXQ0Hm8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd00f374-2dde-4a2b-9da0-e5def3fed650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0757 - accuracy: 0.9758\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc = network.evaluate(test_images, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6QizHgB0Hm-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70db6f30-eda4-40da-f977-801e0cef75d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_acc: 0.9757999777793884\n"
          ]
        }
      ],
      "source": [
        "print('test_acc:', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEzOBPol0HnA"
      },
      "source": [
        "\n",
        "Our test set accuracy turns out to be 97.8% -- that's quite a bit lower than the training set accuracy. \n",
        "This gap between training accuracy and test accuracy is an example of \"overfitting\", \n",
        "the fact that machine learning models tend to perform worse on new data than on their training data. \n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
        "\n",
        "## <font color='#EE4C2C'>2. Accuracy with 7 epochs</font> \n",
        "What is the accuracy on our test data if we use 7 epochs?  You will need to build, compile, and fit a new model.\n",
        "\n",
        "### Build"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2awmR340HnA"
      },
      "outputs": [],
      "source": [
        "network2 = models.Sequential()\n",
        "network2.add(layers.Dense(hidden_size, activation='relu', input_shape=(input_size,)))\n",
        "network2.add(layers.Dense(output_size, activation='softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YdSlKfSwkFP"
      },
      "source": [
        "### Compile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egIaBbbSwmLd"
      },
      "outputs": [],
      "source": [
        "network2.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-ADsdlAwmnM"
      },
      "source": [
        "### Fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWytfea1wreu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b119eb9d-d674-4367-8463-e9b97c19ff04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.2953 - accuracy: 0.9158\n",
            "Epoch 2/7\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.1303 - accuracy: 0.9620\n",
            "Epoch 3/7\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0888 - accuracy: 0.9735\n",
            "Epoch 4/7\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0678 - accuracy: 0.9797\n",
            "Epoch 5/7\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0527 - accuracy: 0.9844\n",
            "Epoch 6/7\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0427 - accuracy: 0.9869\n",
            "Epoch 7/7\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0340 - accuracy: 0.9902\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8137473f10>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "network2.fit(train_images, train_labels, epochs=7, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM7ibNFWwtoi"
      },
      "source": [
        "### Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TZucb2gwxK-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d90e450-56d9-4eb8-83fb-76dd2507dc84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0699 - accuracy: 0.9790\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9789999723434448"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "test2_loss, test2_accuracy = network2.evaluate(test_images, test_labels)\n",
        "test2_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjZOO2O10HnD"
      },
      "source": [
        "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
        "\n",
        "## <font color='#EE4C2C'>3. Accuracy without training</font> \n",
        "What is the accuracy on our test data using our network before we do any training (before `fit`)? You will need to build, and compile a new model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKR6VDnd0HnD"
      },
      "outputs": [],
      "source": [
        "# build\n",
        "network3 = models.Sequential()\n",
        "network3.add(layers.Dense(hidden_size, activation='relu', input_shape=(input_size,)))\n",
        "network3.add(layers.Dense(output_size, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdP1DMxjxB1H"
      },
      "outputs": [],
      "source": [
        "#compile\n",
        "network3.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "test3_loss, test3_accuracy = network3.evaluate(test_images, test_labels)\n",
        "test3_accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JC2tuzlIhwrs",
        "outputId": "631bee0c-c377-4309-f294-42514908ebe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 2.4151 - accuracy: 0.1068\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10679999738931656"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's 6% accurate lol"
      ],
      "metadata": {
        "id": "cBNWqR0QiHz8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4qA1lPZ0HnG"
      },
      "source": [
        "### Training\n",
        "As you can see from your above experiment, before we fit the model the accuracy was not very good. Before training, the weight are set at random (not exactly, but let's keep it simple for now). \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![](http://zacharski.org/files/courses/cs419/deepLearning.png)\n",
        "\n",
        "When we were fitting the model:\n",
        "\n",
        "     network.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
        "    \n",
        "we processed 128 images at a time (this is called batch_size). Everytime we processed the images in that batch we calculated the loss and adjusted the weights to improve the network's  performance. Each time we go through the entire dataset we call it an epoch. So in our initial training, we went through the dataset 5 times.  Once we have gone through the data 5 times we stop and now we have a trained neural network. 'Trained' simply means that we have a network with the weight adjusted to reduce loss.\n",
        "\n",
        "#### Epochs \n",
        "Again, epochs are how many times we go through the training data. You may wonder, are more epochs always better. Stop for a moment and ponder this. \n",
        "\n",
        "* .\n",
        "* .\n",
        "* .\n",
        "* .\n",
        "If you need a clue let me mention bias and variance. \n",
        "\n",
        "In the next notebook we will be displaying both the accuracy on the training data and the accuracy on the validation error.  If at some point you see the training data accuracy still going gradually down, but the validation error going up. You may be overfitting your data.\n",
        "\n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
        "\n",
        "## <font color='#EE4C2C'>4. Fashion</font> \n",
        "\n",
        "As a small first step try out the FashionMNIST dataset.\n",
        "\n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/clothing.gif)\n",
        "\n",
        "\n",
        "The dataset consists of small 28x28 grayscale image icons of different articles of clothing. There are 60,000 images in the training set and 10,000 in the test set. Each image has an associated label from a list of 10:\n",
        "\n",
        "\n",
        "| Label | Description |\n",
        "| --- | --- |\n",
        "| 0 | T-shirt/top |\n",
        "| 1 | Trouser |\n",
        "| 2 | Pullover |\n",
        "| 3 | Dress |\n",
        "| 4 | Coat |\n",
        "| 5 | Sandal |\n",
        "| 6 | Shirt |\n",
        "| 7 | Sneaker |\n",
        "| 8 | Bag |\n",
        "| 9 | Ankle boot |\n",
        "\n",
        "\n",
        "\n",
        "#### The files\n",
        "\n",
        "* Training set: [clothes_train.csv](http://zacharski.org/files/courses/cs419/clothes_train.csv)\n",
        "* Test set: [clothing_test.csv](http://zacharski.org/files/courses/cs419/cTest.csv) Note: Don't use the test set for training.\n",
        "\n",
        "Can you create a network with one hidden layer similar to the example above and train it?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DZJmQ-g0HnG"
      },
      "outputs": [],
      "source": [
        "# I downloaded the csvs and add them to the workspace here in colab\n",
        "import pandas as pd\n",
        "clothes_train = pd.read_csv('clothes_train.csv')\n",
        "clothes_train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clothes_test = pd.read_csv('clothes_test.csv')\n",
        "clothes_test"
      ],
      "metadata": {
        "id": "majZgGck5IJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to make sure that the max is 255\n",
        "import numpy as np\n",
        "max_array = np.array(clothes_train.max())\n",
        "max_array = np.append(max_array, np.ravel(np.array(clothes_test.max())))\n",
        "np.amax(max_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcyMHybmcj4w",
        "outputId": "0c225a96-98c5-45a9-ef96-4c99d8bd828e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "255"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Divide both datasets into img and labels\n",
        "\n",
        "#labels\n",
        "clothes_train_labels = [clothes_train['label']]\n",
        "clothes_test_labels = clothes_test['label']"
      ],
      "metadata": {
        "id": "xjgr1_a53xNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to convert to 1d vector\n",
        "clothes_train_labels = (np.array(clothes_train_labels)).flatten()\n",
        "\n",
        "clothes_test_labels = (np.array(clothes_test_labels)).flatten()"
      ],
      "metadata": {
        "id": "DAVfJn6WsMqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#img\n",
        "\n",
        "column_list = clothes_train.columns\n",
        "column_list = column_list.drop('label')\n",
        "# column_list\n",
        "\n",
        "clothes_train_img = clothes_train[column_list]\n",
        "clothes_train_img = np.array(clothes_train_img)\n",
        "\n",
        "clothes_test_img = clothes_test[column_list]\n",
        "clothes_test_img = np.array(clothes_test_img)\n",
        "\n",
        "clothes_test_img"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCGrrPnS8Xdk",
        "outputId": "1767867e-64ce-4377-bbe3-899c21c32ffd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 1, 0],\n",
              "       [0, 1, 3, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#reshape\n",
        "clothes_train_img = clothes_train_img.reshape((60000, 28*28))\n",
        "clothes_train_img = clothes_train_img.astype('float32')/255\n",
        "\n",
        "clothes_test_img = clothes_test_img.reshape((10000, 28*28))\n",
        "clothes_test_img = clothes_test_img.astype('float32')/255"
      ],
      "metadata": {
        "id": "9j-THNa3blm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clothes_test_img[0]"
      ],
      "metadata": {
        "id": "WjGawaidjxJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build the network\n",
        "input_size = 784\n",
        "hidden_size = 256\n",
        "output_size = 10\n",
        "\n",
        "network_cl = models.Sequential()\n",
        "network_cl.add(layers.Dense(hidden_size, activation='relu', input_shape=(input_size, )))\n",
        "network_cl.add(layers.Dense(output_size, activation='softmax'))"
      ],
      "metadata": {
        "id": "2L3JeiK0-p_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compile\n",
        "\n",
        "# I'm using the loss='SparseCategoricalCrossentropy' since there are two or more label classes, and they are provided as ints\n",
        "# Also, I took a quick loook at the different types of optimizers and decided to try out adam\n",
        "network_cl.compile(optimizer='adam',\n",
        "                   loss='SparseCategoricalCrossentropy',\n",
        "                   metrics=['sparse_categorical_accuracy'])"
      ],
      "metadata": {
        "id": "VClr4H8-EFE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train\n",
        "#I read that with a large numbers of epochs the model can get better tuned, so I tried epochs = 50, 100, 500\n",
        "#The three of them gave accuracy numbers around 90%\n",
        "network_cl.fit(clothes_train_img, clothes_train_labels, epochs=50, batch_size=256)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-qnv3DBUsn9",
        "outputId": "1c414e82-3766-4d08-aaf3-edab1da1f098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.5811 - sparse_categorical_accuracy: 0.8016\n",
            "Epoch 2/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.4137 - sparse_categorical_accuracy: 0.8559\n",
            "Epoch 3/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.3749 - sparse_categorical_accuracy: 0.8664\n",
            "Epoch 4/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.3469 - sparse_categorical_accuracy: 0.8770\n",
            "Epoch 5/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.3275 - sparse_categorical_accuracy: 0.8828\n",
            "Epoch 6/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.3126 - sparse_categorical_accuracy: 0.8879\n",
            "Epoch 7/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.2957 - sparse_categorical_accuracy: 0.8934\n",
            "Epoch 8/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.2870 - sparse_categorical_accuracy: 0.8965\n",
            "Epoch 9/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.2761 - sparse_categorical_accuracy: 0.9008\n",
            "Epoch 10/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.2662 - sparse_categorical_accuracy: 0.9033\n",
            "Epoch 11/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2586 - sparse_categorical_accuracy: 0.9056\n",
            "Epoch 12/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2486 - sparse_categorical_accuracy: 0.9091\n",
            "Epoch 13/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2393 - sparse_categorical_accuracy: 0.9133\n",
            "Epoch 14/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.2325 - sparse_categorical_accuracy: 0.9156\n",
            "Epoch 15/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.2281 - sparse_categorical_accuracy: 0.9168\n",
            "Epoch 16/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.2220 - sparse_categorical_accuracy: 0.9188\n",
            "Epoch 17/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.2135 - sparse_categorical_accuracy: 0.9224\n",
            "Epoch 18/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.2100 - sparse_categorical_accuracy: 0.9227\n",
            "Epoch 19/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.2025 - sparse_categorical_accuracy: 0.9265\n",
            "Epoch 20/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.2003 - sparse_categorical_accuracy: 0.9271\n",
            "Epoch 21/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.1929 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 22/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.1877 - sparse_categorical_accuracy: 0.9310\n",
            "Epoch 23/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.1852 - sparse_categorical_accuracy: 0.9324\n",
            "Epoch 24/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.1759 - sparse_categorical_accuracy: 0.9363\n",
            "Epoch 25/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.1753 - sparse_categorical_accuracy: 0.9374\n",
            "Epoch 26/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.1694 - sparse_categorical_accuracy: 0.9379\n",
            "Epoch 27/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.1651 - sparse_categorical_accuracy: 0.9401\n",
            "Epoch 28/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.1599 - sparse_categorical_accuracy: 0.9419\n",
            "Epoch 29/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.1574 - sparse_categorical_accuracy: 0.9429\n",
            "Epoch 30/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.1522 - sparse_categorical_accuracy: 0.9449\n",
            "Epoch 31/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.1491 - sparse_categorical_accuracy: 0.9459\n",
            "Epoch 32/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.1458 - sparse_categorical_accuracy: 0.9479\n",
            "Epoch 33/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.1449 - sparse_categorical_accuracy: 0.9470\n",
            "Epoch 34/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.1408 - sparse_categorical_accuracy: 0.9492\n",
            "Epoch 35/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.1347 - sparse_categorical_accuracy: 0.9512\n",
            "Epoch 36/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.1310 - sparse_categorical_accuracy: 0.9531\n",
            "Epoch 37/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.1292 - sparse_categorical_accuracy: 0.9530\n",
            "Epoch 38/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.1233 - sparse_categorical_accuracy: 0.9567\n",
            "Epoch 39/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.1268 - sparse_categorical_accuracy: 0.9541\n",
            "Epoch 40/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.1199 - sparse_categorical_accuracy: 0.9578\n",
            "Epoch 41/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.1134 - sparse_categorical_accuracy: 0.9594\n",
            "Epoch 42/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.1123 - sparse_categorical_accuracy: 0.9592\n",
            "Epoch 43/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.1130 - sparse_categorical_accuracy: 0.9599\n",
            "Epoch 44/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.1087 - sparse_categorical_accuracy: 0.9609\n",
            "Epoch 45/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.1053 - sparse_categorical_accuracy: 0.9629\n",
            "Epoch 46/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.1025 - sparse_categorical_accuracy: 0.9634\n",
            "Epoch 47/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.1017 - sparse_categorical_accuracy: 0.9638\n",
            "Epoch 48/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.1007 - sparse_categorical_accuracy: 0.9638\n",
            "Epoch 49/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.0952 - sparse_categorical_accuracy: 0.9660\n",
            "Epoch 50/50\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.0948 - sparse_categorical_accuracy: 0.9667\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8084f02040>"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgnlMXV90HnI"
      },
      "source": [
        "### What is its accuracy on the test data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nR_lX8i90HnJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4141cfc9-bc53-4e48-b9e8-15cf683115fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.3315 - sparse_categorical_accuracy: 0.9010\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9010000228881836"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "loss_cl, accuracy_cl = network_cl.evaluate(clothes_test_img, clothes_test_labels)\n",
        "accuracy_cl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tY3fZL-l0HnL"
      },
      "source": [
        "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
        "\n",
        "## <font color='#EE4C2C'>5. Two Hidden Layers</font> .\n",
        "Can you construct and train a new network that has two hidden layers \n",
        "(as before, the first layer can have 512 neurons - the second should have 256)? What is its accuracy on the test data? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiuUUZs30HnL"
      },
      "outputs": [],
      "source": [
        "#build the network\n",
        "input_size = 784\n",
        "hidden_size_1 = 512\n",
        "hidden_size_2 = 256\n",
        "output_size = 10\n",
        "\n",
        "network_cl2 = models.Sequential()\n",
        "network_cl2.add(layers.Dense(hidden_size_1, activation='relu', input_shape=(input_size, )))\n",
        "network_cl2.add(layers.Dense(hidden_size_2, activation='relu'))\n",
        "network_cl2.add(layers.Dense(output_size, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Compile\n",
        "network_cl2.compile(optimizer='adam',\n",
        "                   loss='SparseCategoricalCrossentropy',\n",
        "                   metrics=['sparse_categorical_accuracy'])"
      ],
      "metadata": {
        "id": "OfYBGG2Azz9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train\n",
        "network_cl2.fit(clothes_train_img, clothes_train_labels, epochs=50, batch_size=256)"
      ],
      "metadata": {
        "id": "IC2zkrz50B6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_cl2, accuracy_cl2 = network_cl2.evaluate(clothes_test_img, clothes_test_labels)\n",
        "accuracy_cl2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeicyL3h3WXt",
        "outputId": "06a6bfab-e696-491b-f43d-60f06b7ec49b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.4692 - sparse_categorical_accuracy: 0.8991\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8991000056266785"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuraccy is about the same. I think the model is overfitted in both cases"
      ],
      "metadata": {
        "id": "7CykH-XY6lRK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8ll6Pz00HnO"
      },
      "source": [
        "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/torchdivide.png)\n",
        "\n",
        "## <font color='#EE4C2C'>6. Volcanos on Venus</font> \n",
        "![](https://upload.wikimedia.org/wikipedia/commons/1/16/Maat_Mons_on_Venus.jpg)\n",
        "\n",
        "\n",
        "### First, no one has been to Venus\n",
        "I felt I needed to say that upfront because we don't really know if there are volcanoes on Venus. An analysis of the data by experts is not 100% accurate so the labels are the experts best guess.\n",
        "\n",
        "The images are from NASA's Magellan spacecraft which was launched on May 4, 1989 and made it to Venus on August 10, 1990. Magellan mapped the surface of Venus using synthetic aperture radar. Some images have black blocks in them caused by either problems with Magellan or with communication back to earth. \n",
        "\n",
        "### The data\n",
        "The images are 110x110 grayscale pixels. The value of each pixel ranges from 0 to 255. Your task it to build a classifier that will predict whether a volcano is present in the image or not.\n",
        "\n",
        "#### The files\n",
        "\n",
        "* Training images: [volcanoes_train_images.csv](http://zacharski.org/files/courses/cs419/volcanoes_train_images.csv)\n",
        "* Training labels: [volcanoes_train_labels.csv](http://zacharski.org/files/courses/cs419/volcanoes_train_labels.csv) \n",
        "* Testing images: [volcanoes_test_images.csv](http://zacharski.org/files/courses/cs419/volcanoes_test_images.csv)\n",
        "* Testing labels: [volcanoes_test_labels.csv](http://zacharski.org/files/courses/cs419/volcanoes_test_labels.csv) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Tasks\n",
        "\n",
        "There are 5 tasks\n",
        "\n",
        "1. A simple task: Can you display a few of the images from the dataset?\n",
        "2. Build a model with one hidden layer. Train it for 2 epochs. What is it's accuracy?\n",
        "3. Build a model with one hidden layer. Train it for 10 epochs. What is it's accuracy? \n",
        "4. Does increasing the epochs beyond 10 improve accuracy?\n",
        "5. Build a model with 2 hidden layers and test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZFUG8Fx0HnO"
      },
      "outputs": [],
      "source": [
        "# Task 1: display a few images from the dataset\n",
        "\n",
        "#load the imgs dfs\n",
        "volc_train_img = pd.read_csv('volcanoes_train_images.csv', header=None).to_numpy()\n",
        "volc_test_img = pd.read_csv('volcanoes_test_images.csv', header=None).to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "volc_train_img.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwT6chEJMrad",
        "outputId": "57a58081-9226-4884-92bd-3dad162ce835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7000, 12100)"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reshape to display the images\n",
        "volc_train_img_display = volc_train_img.reshape(7000, 110, 110)"
      ],
      "metadata": {
        "id": "fRCWQlZrLUag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfaIBXN_0HnQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deca9673-7479-4384-b7e3-f5850f75a265"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7000, 110, 110)"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ],
      "source": [
        "volc_train_img_display.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkepiU9S0HnS"
      },
      "outputs": [],
      "source": [
        "#Display\n",
        "for i in range(0, 5):\n",
        "  viewImage(volc_train_img_display[i])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a model with one hidden layer. Train it for 2 epochs. What is it's accuracy? - Task 2\n",
        "\n",
        "#First, let's preprare the data - img\n",
        "# Btw: the answer to the question at the beginning in the notebook. I think that the division by 255 is in order to normalize \n",
        "# the data with values between 0 and 1\n",
        "\n",
        "volc_train_img = volc_train_img.astype('float32')/255\n",
        "volc_test_img = volc_test_img.astype('float32')/255\n",
        "\n",
        "volc_train_img"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ati-DT23Nabx",
        "outputId": "7a2f5847-1a62-4d21-f48b-91243ad2f4da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00146098, 0.00155325, 0.00152249, ..., 0.00178393, 0.00181469,\n",
              "        0.00147636],\n",
              "       [0.00139946, 0.00141484, 0.00139946, ..., 0.00153787, 0.00138408,\n",
              "        0.00124567],\n",
              "       [0.00133795, 0.00107651, 0.00110727, ..., 0.0012303 , 0.00129181,\n",
              "        0.00138408],\n",
              "       ...,\n",
              "       [0.00204537, 0.00156863, 0.00186082, ..., 0.00199923, 0.00201461,\n",
              "        0.00170704],\n",
              "       [0.00181469, 0.00179931, 0.00176855, ..., 0.00153787, 0.00153787,\n",
              "        0.00167628],\n",
              "       [0.00113802, 0.0013687 , 0.00141484, ..., 0.00193772, 0.00178393,\n",
              "        0.00163014]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the labels\n",
        "volc_train_labels = pd.read_csv('volcanoes_train_labels.csv')\n",
        "volc_test_labels = pd.read_csv('volcanoes_test_labels.csv')\n",
        "volc_train_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "GoqJ-cw0OOr7",
        "outputId": "cc788466-e235-4455-d271-77ae3838b727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Volcano?  Type  Radius  Number Volcanoes\n",
              "0            1   3.0   17.46               1.0\n",
              "1            0   NaN     NaN               NaN\n",
              "2            0   NaN     NaN               NaN\n",
              "3            0   NaN     NaN               NaN\n",
              "4            0   NaN     NaN               NaN\n",
              "...        ...   ...     ...               ...\n",
              "6995         0   NaN     NaN               NaN\n",
              "6996         0   NaN     NaN               NaN\n",
              "6997         0   NaN     NaN               NaN\n",
              "6998         0   NaN     NaN               NaN\n",
              "6999         0   NaN     NaN               NaN\n",
              "\n",
              "[7000 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-71826e15-358e-4b42-8cd3-3640dd3eb787\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Volcano?</th>\n",
              "      <th>Type</th>\n",
              "      <th>Radius</th>\n",
              "      <th>Number Volcanoes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>17.46</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6995</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6996</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6997</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6998</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6999</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7000 rows Ã 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-71826e15-358e-4b42-8cd3-3640dd3eb787')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-71826e15-358e-4b42-8cd3-3640dd3eb787 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-71826e15-358e-4b42-8cd3-3640dd3eb787');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get only the Volcano? column as label\n",
        "volc_train_labels = volc_train_labels['Volcano?'].to_numpy()\n",
        "volc_test_labels = volc_test_labels['Volcano?'].to_numpy()\n",
        "\n",
        "volc_train_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6o1-H_cePDII",
        "outputId": "8a05442e-2b0c-4fee-8ec4-b2a50ab2032a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, ..., 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to have an idea of the number of volcanoes\n",
        "unique, counts = np.unique(volc_train_labels, return_counts=True)\n",
        "dict(zip(unique, counts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0A71XqZrSgMW",
        "outputId": "966d2e36-3f66-4a43-a275-407b89c47af4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 6000, 1: 1000}"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build a model\n",
        "input_size = 12100\n",
        "hidden_size = 512\n",
        "output_size = 1\n",
        "\n",
        "network_volc = models.Sequential()\n",
        "network_volc.add(layers.Dense(hidden_size, activation='relu', \n",
        "                              input_shape=(input_size,)))\n",
        "network_volc.add(layers.Dense(output_size, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "hwNpWhApTa7y"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compile - I tried both: 'adam' and 'rmsprop'. Both accuracy percents were around 85%\n",
        "network_volc.compile(optimizer='adam',\n",
        "                     loss='binary_crossentropy',\n",
        "                     metrics=['binary_accuracy'])"
      ],
      "metadata": {
        "id": "sn22U1ETV6g5"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train - epochs = 2\n",
        "network_volc.fit(volc_train_img, volc_train_labels, epochs=2, batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niBilOWGWqeI",
        "outputId": "d2ee72ea-45fa-49c0-da7d-4b06c53aa1c1"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "55/55 [==============================] - 1s 7ms/step - loss: 0.4435 - binary_accuracy: 0.8571\n",
            "Epoch 2/2\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 0.4175 - binary_accuracy: 0.8571\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8065a5f3a0>"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#accuracy\n",
        "volc_loss, volc_baccuracy = network_volc.evaluate(volc_test_img, volc_test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dypkns1OW7sY",
        "outputId": "2ec138d8-7cfb-4c92-ea7c-a114ab11898f"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86/86 [==============================] - 0s 3ms/step - loss: 0.4469 - binary_accuracy: 0.8413\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a model with one hidden layer. Train it for 10 epochs. What is it's accuracy? - Task 3\n",
        "\n",
        "# build a model\n",
        "network_volc2 = models.Sequential()\n",
        "network_volc2.add(layers.Dense(hidden_size, activation='relu', \n",
        "                              input_shape=(input_size,)))\n",
        "network_volc2.add(layers.Dense(output_size, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "sKpiqgb5XPD4"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compile \n",
        "network_volc2.compile(optimizer='adam',\n",
        "                     loss='binary_crossentropy',\n",
        "                     metrics=['binary_accuracy'])"
      ],
      "metadata": {
        "id": "FnmuBG9Lmbjs"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train - epochs = 10\n",
        "network_volc2.fit(volc_train_img, volc_train_labels, epochs=10, batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBct_QXtmjVX",
        "outputId": "613347cb-3905-4a91-db60-785d0f5a06e6"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "55/55 [==============================] - 1s 8ms/step - loss: 0.4403 - binary_accuracy: 0.8436\n",
            "Epoch 2/10\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 0.4173 - binary_accuracy: 0.8571\n",
            "Epoch 3/10\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 0.4134 - binary_accuracy: 0.8571\n",
            "Epoch 4/10\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 0.4095 - binary_accuracy: 0.8571\n",
            "Epoch 5/10\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 0.4069 - binary_accuracy: 0.8571\n",
            "Epoch 6/10\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 0.4023 - binary_accuracy: 0.8571\n",
            "Epoch 7/10\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 0.3973 - binary_accuracy: 0.8571\n",
            "Epoch 8/10\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 0.3958 - binary_accuracy: 0.8571\n",
            "Epoch 9/10\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 0.3908 - binary_accuracy: 0.8571\n",
            "Epoch 10/10\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 0.3861 - binary_accuracy: 0.8571\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8039b7b1f0>"
            ]
          },
          "metadata": {},
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#accuracy\n",
        "volc_loss2, volc_baccuracy2 = network_volc2.evaluate(volc_test_img, volc_test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0ohq1IUnHB1",
        "outputId": "acbe5de9-affc-47bf-a719-e24f6c863ee6"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86/86 [==============================] - 0s 3ms/step - loss: 0.4088 - binary_accuracy: 0.8413\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Does increasing the epochs beyond 10 improve accuracy? Not, really. The accuracy is the same for epochs=2 and epochs=10. From my experience, running up to 500 epochs on a previous model, the difference in epochs between 2 and 10 doesn't produce a significant increase in accuracy."
      ],
      "metadata": {
        "id": "NaqDlkaXpK9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a model with 2 hidden layers and test. - Task 4\n",
        "# build a model\n",
        "network_volc3 = models.Sequential()\n",
        "network_volc3.add(layers.Dense(hidden_size, activation='relu', \n",
        "                              input_shape=(input_size,)))\n",
        "network_volc3.add(layers.Dense(hidden_size*2, activation='relu'))\n",
        "network_volc3.add(layers.Dense(output_size, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "Pnzug1gMo8i5"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compile \n",
        "network_volc3.compile(optimizer='adam',\n",
        "                     loss='binary_crossentropy',\n",
        "                     metrics=['binary_accuracy'])"
      ],
      "metadata": {
        "id": "WNAI_QzOuYrT"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train\n",
        "network_volc3.fit(volc_train_img, volc_train_labels, epochs=50, batch_size=128)"
      ],
      "metadata": {
        "id": "9sK2fN3euf2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#accuracy - much better that in previous volcanoes models\n",
        "volc_loss3, volc_baccuracy3 = network_volc3.evaluate(volc_test_img, volc_test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afIGaxUKxLum",
        "outputId": "2b88e32a-778f-4f05-b60e-7d2901521a30"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86/86 [==============================] - 0s 3ms/step - loss: 0.2146 - binary_accuracy: 0.9232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oPtUCBi0HnU"
      },
      "source": [
        "#### Remix\n",
        "Remix by Ron Zacharski. Orginal Python notebook by FranÃ§ois Chollet\n",
        "\n",
        "### MIT License\n",
        "\n",
        "Copyright (c) 2017 FranÃ§ois Chollet\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}